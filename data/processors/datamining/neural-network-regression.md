# ![ ](../../images/icons/components/regressionneuralnet_default.svg) Neural network (regression)

Решает задачу [регрессии](https://wiki.loginom.ru/articles/regression-line.html) — в выходном наборе [*Нейросеть*](https://basegroup.ru/deductor/function/algorithm/neuronet) выдаст прогнозируемое значение для одной или нескольких переменных, зависимых от множества входных параметров.

Перед тем, как производить прогноз, алгоритм обучается на тренировочном наборе данных — обучающей выборке. Each row of such sample contains the following data:

* a set of input parameters in the fields marked as the **input** ones;
* в полях, обозначаемых как **выходные** — соответствующие входным параметрам значения зависимых переменных.

Технически обучение заключается в нахождении *весов* - коэффициентов связей между нейронами. In the process of training, the neural network enables to detect complex dependences between input and output parameters, and also to perform generalization. Это значит, что в случае успешного обучения *Нейросеть* способна выдать верный результат на основании данных, которые отсутствовали в обучающей выборке, а также на неполных и/или «зашумленных», частично искажённых данных. Для обучения используется квазиньютоновский [метод Бройдена-Флетчера-Гольдфарба-Шанно](https://ru.wikipedia.org/wiki/Алгоритм_Бройдена_—_Флетчера_—_Гольдфарба_—_Шанно) с ограниченным использованием памяти L-BFGS.

В задаче регрессии (в отличии от [задачи классификации](../../processors/datamining/neural-network-classification.md)) **выходными** могут быть только поля с непрерывным [видом данных](../../data/datatype.md). Вид данных входных полей не регламентируется.

----

**Примечание:** Для каждого непрерывного параметра в структуре *Нейросети* будет создан один вход, в то время как для каждого дискретного – столько входов, сколько у данного параметра имеется различных уникальных значений.

----

## Ports

### Input

* ![ ](../../images/icons/app/node/ports/inputs/table_inactive.svg) — **Input data source** (data table).

#### Requirements to the Received Data

The input data set fields that will be used as the **input** or **output** ones, must not contain null values. If this requirement is not met, error message will appear when node activating.

### Output

* ![ ](../../images/icons/app/node/ports/outputs/table_inactive.svg) — **Neural network output** (data table). Also refer to [output data set structure](./neural-network-classification/output-set.md).
* ![ ](../../images/icons/app/node/ports/outputs/variable_inactive.svg) — **Model quality indicators**.

## Wizard

### Step 1. Usage Type of Input Columns

It is required to set the [usage type](../../data/datasetfieldoptions.md) of the input data set fields at the first stage.
It is required to select one of the following usage types for each of the fields:

* ![ ](../../images/icons/usage-types/active_default.svg) **Входное** — поле содержит значения одного из входных параметров.
* ![ ](../../images/icons/usage-types/predicted_default.svg) **Выходное** — поле содержит значения классов.
* ![ ](../../images/icons/usage-types/unspecified_default.svg) **Unused**: the field is not included into processing. It is set for other fields by default.

### Step 2. Configuration of normalization

На этом этапе входные данные приводятся к [нормальному виду](../normalization/README.md) - преобразуются из натуральных значений в безразмерные для того, чтобы данные, имеющие большой разброс значений, не превалировали над данными с меньшим разбросом значений. Использование [нормализации](https://wiki.loginom.ru/articles/normalization.html) увеличивает качество и скорость обучения *Нейросети*.

### Step 3. Partitioning

На этом этапе входные данные можно разделить на [тестовое](https://wiki.loginom.ru/articles/test-set.html), [обучающее](https://wiki.loginom.ru/articles/training-set.html) и [валидационное](https://wiki.loginom.ru/articles/validation-set.html) множества (выборки).

#### Partition method

* Случайный - данные для тестового и обучающего множеств формируется из всего объема входных данных в случайном порядке и случайной пропорции.

* Последовательный - соотношение данных для тестового и обучающего множеств задается вручную. При этом есть возможность выбрать очередность, по которой сперва выбираются данные для тестового множества, потом для обучающего, или наоборот.

#### Validation Method

* Без валидации - [валидация](../validation.md) обучения Нейросети не производится.

* K-fold [кросс-валидация](https://wiki.loginom.ru/articles/cross-validation.html) - входные данные разбиваются на К колод (частей). Нейросеть обучается на основе всех колод кроме одной, которая  используется для валидации. Обучение повторяется K раз, при этом на каждой итерации для валидации используется новая колода.

* [Монте-Карло](https://wiki.loginom.ru/articles/monte-carlo-technique.html) - входные данные случайным образом разделяются на обучающее и валидационное множество в соответствии с заданной пропорцией. Обучение повторяется в соответствии с заданным количеством итераций ресемплинга, при этом для каждой итерации формируется новое валидационное множество (ресемпл).

### Step 4. Configure Neural Network Parameters

#### Neural Network Structure

* Number of hidden layers is selected from the list:
   * No hidden layers.
   * One hidden layer (used by default).
   * Two hidden layers.
* Количество нейронов в первом скрытом слое — целое число >= 1 (по умолчанию = 10).
* Number of neurons in the second hidden layer — integer >= 1 (by default = 10).

#### Output Value Limit

Тип ограничения определяет форму [активационной функции](https://wiki.loginom.ru/articles/activation-function.html) выходного слоя Нейросети:

* Нет — линейная.
* Интервал — гиперболический тангенс.
* Только снизу — усеченная снизу экспонента.
* Только сверху — усеченная сверху экспонента.

Также возможно задать верхнюю и нижнюю границу ограничения формы активационной функции. Как правило, этот параметр напрямую коррелирует от параметра нормализации входных данных.

#### Training Parameters

* Количество рестартов — число попыток обучения *Нейросети* (на одном и том же наборе), выполняемых из случайных начальных значений весов сети. Upon completion of all restarts, it is required to select the network that provides the least root-mean-square error of the training set. Должно быть целым числом >= 1 (по умолчанию = 10).
* Степень регуляризации — степень зависимости весов сети друг от друга. The higher this dependence, the stronger impact exerts one input parameter on the other ones. The decay enables to decrease the effective number of the model degrees of freedom, thereby avoiding overfitting. The following options are available:
   * Отсутствует (0).
   * Very weak (20).
   * Weak (40).
   * Mean (60).
   * Strong (80).
   * Very strong (100).
* Продолжить обучение — установление данного флага позволяет начать переобучение модели не со случайных значений весов Нейросети, а с полученных при последнем обучении. In this case, the "Number of restarts" parameter is ignored.

#### Stop Criteria

The network is trained in the iterative manner. При каждой итерации считывается весь обучающий набор данных и изменяются веса *Нейросети*. Этот процесс продолжается пока относительные изменения весов не станут меньше заданного порога или количество итераций не превысит заданной величины.

* Порог минимального изменения весов — если на очередном шаге обучения относительное изменение нормы вектора весов становится меньше порога, то обучение останавливается. By default = 0.01.
* Максимальное количество эпох — максимальное количество итераций обучения алгоритма. Этот параметр по умолчанию отключен. Если процесс обучения необходимо ограничить по времени, в этом случае он остановится после заданного количества эпох, даже если обучение еще не пришло к оптимальной точке, т.е. не достигнут порог минимального изменения весов.

### Step 3. Configure Auto Selection of Neural Network Parameters

Three structure related parameters can be selected for the neural network:

* Number of hidden layers (0, 1 or 2);
* Number of neurons in each hidden layer;
* Decay degree enables to adjust the model stiffness.

#### Common Parameters

* **Structure autofit** provides the auto selection of the Neural Network structure:
   * **Начать с указанной структуры** — использование в качестве начальных параметров значений, заданных на странице настройки параметров Нейросети (см. Step 2).
* **Подобрать степень регуляризации** — автоматический подбор степени регуляризации Нейросети:
   * **Начать с указанной степени регуляризации** — использование в качестве начальной *Степени регуляризации* значения, заданного на странице настройки параметров Нейросети.

> **Примечание:** если необходимо осуществлять подбор параметров для больших входных объемов или сложных моделей, можно включить только подбор структуры либо только степени регуляризации, сократив время на обучение.

#### Sampling Parameters

Для ускорения процесса автоподбора предусмотрено задание подвыборки, на которой он будет производиться:

* **Использовать подмножество обучающего набора** — использование подвыборки [обучающего множества](https://wiki.loginom.ru/articles/training-set.html) для автоподбора;
   * **Percent sample size** means the size of the training set subsample.
   * **Maximum sample size** means the maximum size of the training set subsample.

#### Auto Stop Criteria

По умолчанию процесс автоматического подбора останавливается при невозможности найти лучшие параметры, чем уже найденные. Для ограничения времени работы предусмотрена возможность ограничить, в том числе одновременно: количество шагов автоподбора и время автоподбора.

* **Autofit stages not more** means the maximum number of the algorithm steps (0 — restrictions are disabled);
* **Время автоподбора не более (сек.)** — максимальное время работы алгоритма (0 — отключение ограничения).

> **Примечание:** при работе следует учитывать, что фактически оба ограничения могут быть незначительно превышены при использовании подвыборки для автоподбора, так как последним этапом, который не учитывается ограничениями, будет осуществлено обучение лучшей Нейросети на полном наборе.

Отдельный случай останова оптимизатора — если достигнут теоретически наилучший результат. Как для регрессионных сетей, так и для классификатора это равная 0 среднеквадратическая ошибка на обучающем множестве.

Для классификатора также по умолчанию включен *Останов при нулевой ошибки классификации*:

* **Останов при нулевой ошибки классификации** — прекращение автоподбора при достижении нулевой ошибки классификации.

> **Примечание:** опцию *Останов при нулевой ошибки классификации* можно отключить,
> т.к. правильная классификация всех примеров не всегда означает наилучшую структуру Нейросети: оптимизатору можно дать возможность подобрать сеть с лучшей обобщающей способностью (например, с меньшим числом нейронов или сильнее регуляризованную), но при этом не обязательно с нулевой ошибкой классификации.

#### Стратегия оптимизации

Целевой функцией для оптимизатора является среднеквадратическая ошибка на обучающем наборе. При этом для учета случаев, когда несколько сетей показывают сравнимые по точности результаты, в целях выбора сети с самой простой структурой значение целевой функции дополнительно штрафуется на слабо отличающийся от единицы множитель (1+1e-8) за каждый скрытый нейрон.

Стратегия оптимизации следующая:

* Если необходимо подобрать только степень регуляризации для заданной структуры:
   1. Если начальная точка не задана, то степень регуляризации подбирается методом "золотого сечения", в противном случае - методом "схождения к вершине".
* Если необходимо подобрать только структуру, не изменяя степень регуляризации
   1. Если не задана начальная структура, то она подбирается в два этапа: сначала происходит выбор количества скрытых слоев (0, 1 или 2), затем, если результат предыдущего этапа не 0, грубо подбирается размер скрытых слоев методом "золотого сечения", причем для 2 скрытых слоев количество нейронов на данном этапе делается одинаковым;
   2. Структура подбирается сразу по всем трем параметрам (число слоев, число нейронов) методом "схождения к вершине" из заданной либо подобранной начальной точки.
* Если необходим автоподбор структуры и регуляризации:
   1. Подбирается структура так же, как и в предыдущем пункте. При этом, если начальное значение регуляризации задано, то используется оно, в противном случае регуляризация отключена.
   2. Если начальное значение регуляризации не было задано, оно подбирается методом "золотого сечения".
   3. Финальный этап автоподбора производится методом "схождения к вершине" по всем четырем параметрам.

Блок-схема (граф переходов) реализованной стратегии автоподбора указана на рисунке ниже.

![Auto Selection Algorithm](./autofit-neural-network-1.svg)