---
description: Компонент Нейросеть (классификация) в Loginom. Решение задачи классификации. Обучающая выборка. Нахождение весов. Метод Бройдена-Флетчера-Гольдфарба-Шанно с ограничением памяти L-BFGS. K-fold кросс-валидация. Монте-Карло. Мастер настройки.
---
# ![ ](./../../images/icons/components/classifierneuralnet_default.svg) Нейросеть (классификация)

## Описание

Компонент решает задачу [классификации](https://wiki.loginom.ru/articles/classification.html) — в выходном наборе [*Нейросеть*](https://wiki.loginom.ru/articles/neural-network.html) соотносит множество [входных параметров](https://wiki.loginom.ru/articles/input-variable.html) (предикторов) с одним из заранее известных [классов](https://wiki.loginom.ru/articles/class.html):

**{** P(1), P(2), P(3) ... P(n) **}** => Class(i) , где P(n) — входной параметр, Class(i) — один из известных классов.

Перед тем, как производить классификацию, алгоритм обучается на тренировочном наборе данных — [обучающей выборке](https://wiki.loginom.ru/articles/training-set.html). Каждая строка такой выборки содержит:

* в полях, обозначаемых как *входные* — множество входных параметров;
* в единственном *выходном* поле — соответствующее этому набору обозначение класса.

Таким образом, перечень классов задается обучающим набором данных в процессе обучения *Нейросети* и не может быть изменен/пересмотрен в процессе классификации.

Технически обучение заключается в нахождении *весов* — коэффициентов связей между нейронами. В процессе обучения нейронная сеть способна выявлять сложные зависимости между входными и [выходными параметрами](https://wiki.loginom.ru/articles/output-variable.html), а также выполнять обобщение. Это значит, что в случае успешного обучения сеть сможет вернуть верный результат на основании данных, которые отсутствовали в обучающей выборке, а также неполных и/или «зашумленных», частично искажённых данных. Для обучения используется квазиньютоновский [метод Бройдена-Флетчера-Гольдфарба-Шанно](https://ru.wikipedia.org/wiki/Алгоритм_Бройдена_—_Флетчера_—_Гольдфарба_—_Шанно) с ограниченным использованием памяти *L-BFGS*.

В задаче классификации (в отличии от [задачи регрессии](./../../processors/datamining/neural-network-regression.md)) выходным может быть только поле с дискретным [видом данных](./../../data/datakind.md). Вид данных входных полей не регламентируется, они могут быть как непрерывными, так и дискретными.

> **Примечание:** для каждого непрерывного параметра в структуре *Нейросети* будет создан один вход, в то время как для каждого дискретного – столько входов, сколько у данного параметра имеется различных уникальных значений.

## Порты

### Вход

* ![ ](./../../images/icons/app/node/ports/inputs/table_inactive.svg) Входной источник данных (таблица данных).

#### Требования к принимаемым данным

Поля входного набора, которые будут использоваться в качестве *входных* или *выходных*, не должны содержать пропущенные значения. Если это требование не выполнено, то в момент активации узла будет выдана ошибка.

### Выход

* ![ ](./../../images/icons/app/node/ports/outputs/table_inactive.svg) [Выход нейросети](./neural-network-classification/output-set.md) (таблица данных).
* ![ ](./../../images/icons/app/node/ports/outputs/variable_inactive.svg) [Сводка](./neural-network-classification/report.md) (переменные) — показатели качества модели.

## Мастер настройки

### Шаг 1. Настройка входных столбцов

На первом этапе необходимо задать [назначение](./../../data/datasetfieldfeatures.md) полей входного набора данных:

* ![ ](./../../images/icons/common/usage-types/active_default.svg) **Входное** — поле содержит значения одного из входных параметров.
* ![ ](./../../images/icons/common/usage-types/predicted_default.svg) **Выходное** — поле содержит значения классов.
* ![ ](./../../images/icons/common/usage-types/unspecified_default.svg) **Не задано** — поле не участвует в обработке. Устанавливается по умолчанию для прочих полей.

### Шаг 2. Настройка нормализации

На этом этапе входные данные приводятся к нормальному виду — преобразуются из натуральных значений в безразмерные для того, чтобы данные, имеющие большой разброс значений, не превалировали над данными с меньшим разбросом значений. Использование [нормализации](./../normalization/README.md) увеличивает качество и скорость обучения *Нейросети*.

### Шаг 3. Разбиение на множества

Страница *Разбиение на множества* позволяет разделить множество на обучающее и тестовое:

* [Обучающее](https://wiki.loginom.ru/articles/training-set.html) — cтруктурированный набор данных, применяемый для обучения [аналитических моделей](https://wiki.loginom.ru/articles/taught-model.html). Каждая запись обучающего множества представляет собой обучающий пример, содержащий заданное входное воздействие и соответствующий ему правильный выходной (целевой) результат.
* [Тестовое](https://wiki.loginom.ru/articles/test-set.html) — подмножество обучающей выборки, содержащее тестовые примеры, т.е. примеры, использующиеся не для обучения модели, а для проверки его результатов.

Доступные параметры:

* Размер обучающего и тестового множества в процентах или строках.
* Метод разбиения на обучающее и тестовое множество. Существует три метода разбиения:
  * Случайный — случайно разбивает множество записей на обучающее и тестовое множество.
  * Последовательный — группы строк множеств (обучающее, неиспользуемое, тестовое) выбираются последовательно, т.е. сначала выбираются те записи, которые входят в первое множество, затем — во второе и т.д. Порядок множеств можно менять (кнопки *Сдвинуть вверх*, *Сдвинуть вниз*).
  * По столбцу — разбиение на обучающее и тестовое множества задаётся при помощи параметра. Параметром выступает столбец с логическим типом данных, где значение &laquo;ИСТИНА&raquo; указывает на то, что запись относится к тестовому набору, а значение &laquo;ЛОЖЬ&raquo; — на то, что запись принадлежит обучающему набору (т.е. можно разбить множество на обучающее и тестовое в узле [Разбиение на множества](../preprocessing/partitioning.md) и подать данные из порта *Общий выходной набор* на вход узла *Нейросеть (классификация)*, выбрав в качестве параметра разбиения по столбцу колонку "Тестовое множество"). При выборе данного метода таблица выбора соотношения обучающего и тестового множеств становится неактивной.
* Метод [валидации](./../validation.md), который может принимать следующие значения:
  * Без валидации.
  * [K-fold кросс-валидация](https://wiki.loginom.ru/articles/cross-validation.html) — позволяет выбрать *Метод [сэмплинга](https://wiki.loginom.ru/articles/sampling.html)* и количество *Колод кросс-валидации*.
  * [Монте-Карло](https://wiki.loginom.ru/articles/monte-carlo-technique.html?q=) — позволяет выбрать *Количество итераций ресемплинга* и задать размер обучающего и [валидационного множества](https://wiki.loginom.ru/articles/validation-set.html).
  
**Random seed** — начальное число (целое, положительное), которое используется для инициализации генератора псевдослучайных чисел. Последовательность чисел генератора полностью определяется начальным числом. Если генератор повторно инициализируется с тем же начальным числом, он выдаст ту же последовательность чисел.

Параметр влияет на порядок случайного разбиения на тестовое и обучающее множество и на воспроизводимость результата обучения. Можно повторить результат обучения узла, если подать те же данные и выставить тот же random seed.

Для параметра доступны следующие команды:

* Всегда случайно — начальное число всегда будет случайным.
* Генерировать — сгенерируется новое начальное число.
* Копировать — в буфер обмена будет скопировано указанное значение.

### Шаг 4. Настройка параметров Нейросети

#### Структура Нейросети

* **Количество скрытых слоев** — предоставляется выбор из списка:
  * Без скрытых слоев.
  * Один скрытый слой (используется по умолчанию).
  * Два скрытых слоя.
* **Количество нейронов в первом скрытом слое** — целое число >= 1 (по умолчанию = 10).
* **Количество нейронов во втором скрытом слое** — целое число >= 1 (по умолчанию = 10).

#### Параметры обучения

* **Количество рестартов** — число попыток обучения *Нейросети* (на одном и том же наборе), выполняемых из случайных начальных значений весов. По завершении всех рестартов выбирается сеть, которая обеспечивает наименьшую среднеквадратическую ошибку на обучающем множестве. Целое число >= 1 (по умолчанию = 10).
* **Степень регуляризации** — степень зависимости весов сети друг от друга. Чем больше эта зависимость, тем сильнее будет влияние одного входного параметра на другие. Регуляризация позволяет снизить эффективное число степеней свободы модели, избежав тем самым переобучения. Предоставляется выбор из следующих вариантов:
  * Отсутствует (0).
  * Очень слабая (20).
  * Слабая (40). Используется по умолчанию.
  * Средняя (60).
  * Сильная (80).
  * Очень сильная (100).
* **Продолжить обучение** — установка данного флага позволяет начать переобучение модели не со случайных значений весов *Нейросети*, а с полученных при последнем обучении. При этом параметр *Количество рестартов* игнорируется.

#### Критерии останова

Обучение сети происходит итерационно. При каждой итерации считывается весь обучающий набор данных и изменяются веса *Нейросети*. Это продолжается до тех пор, пока относительные изменения весов не станут меньше заданного порога или количество итераций не превысит заданной величины.

* **Порог минимального изменения весов** — если на очередном шаге обучения относительное изменение нормы вектора весов становится меньше порога, то обучение останавливается. По умолчанию = 0,005.
* **Максимальное количество эпох** — максимальное количество итераций обучения алгоритма. Если процесс обучения необходимо ограничить по времени, в этом случае он остановится после заданного количества эпох, даже если обучение еще не пришло к оптимальной точке, т.е. не достигнут порог минимального изменения весов.

### Шаг 5. Настройка автоматического подбора параметров Нейросети

Нейросеть имеет три подбираемых параметра, относящихся к структуре:

* Количество скрытых слоев (0, 1 или 2);
* Количество нейронов в каждом из скрытых слоев;
* **Степень регуляризации** — параметр, регулирующий жесткость модели.

#### Общие параметры

* **Подобрать структуру** — автоматический подбор структуры *Нейросети*:
  * **Начать с указанной структуры** — использование в качестве начальных параметров значений, заданных на странице настройки параметров *Нейросети*.
* **Подобрать степень регуляризации** — автоматический подбор степени регуляризации *Нейросети*:
  * **Начать с указанной степени регуляризации** — использование в качестве начальной *Степени регуляризации* значения, заданного на странице настройки параметров *Нейросети*.

> **Примечание:** если необходимо осуществлять подбор параметров для больших входных объемов или сложных моделей, можно включить только подбор структуры, либо только подбор степени регуляризации, сократив время на обучение.

#### Параметры выборки

Для ускорения процесса автоподбора предусмотрено задание подвыборки, на которой он будет производиться:

* **Использовать подмножество обучающего набора** — использование подвыборки обучающего множества для автоподбора.
  * **Размер выборки в процентах** — размер подвыборки обучающего множества.
  * **Максимальный размер выборки** — максимальный размер подвыборки обучающего множества.

#### Критерии останова автоподбора

По умолчанию процесс автоматического подбора останавливается при невозможности найти лучшие параметры, чем уже найденные. Для ограничения времени работы предусмотрена возможность ограничить, в том числе одновременно количество шагов автоподбора и время автоподбора:

* **Шагов автоподбора не более** — максимальное количество шагов алгоритма (0 — отключение ограничения);
* **Время автоподбора не более (сек.)** — максимальное время работы алгоритма (0 — отключение ограничения).

>**Примечание:** при работе следует учитывать, что фактически оба ограничения могут быть незначительно превышены при использовании подвыборки для автоподбора, так как последним этапом, который не учитывается ограничениями, будет осуществлено обучение лучшей *Нейросети* на полном наборе.

Отдельный случай останова оптимизатора — если достигнут теоретически наилучший результат. Как для регрессионных сетей, так и для классификатора это равная 0 среднеквадратическая ошибка на обучающем множестве.

Для классификатора также по умолчанию включен *Останов при нулевой ошибки классификации*:

* **Останов при нулевой ошибки классификации** — прекращение автоподбора при достижении нулевой ошибки классификации.

>**Примечание:** опцию *Останов при нулевой ошибки классификации* можно отключить,
т.к. правильная классификация всех примеров не всегда означает наилучшую структуру *Нейросети*: оптимизатору можно дать возможность подобрать сеть с лучшей обобщающей способностью (например, с меньшим числом нейронов или сильнее регуляризованную), но при этом не обязательно с нулевой ошибкой классификации.

#### Стратегия оптимизации

Целевой функцией для оптимизатора является среднеквадратическая ошибка на обучающем наборе. При этом для учета случаев, когда несколько сетей показывают сравнимые по точности результаты, в целях выбора сети с самой простой структурой значение целевой функции дополнительно штрафуется на слабо отличающийся от единицы множитель (1+1e-8) за каждый скрытый нейрон.

Стратегия оптимизации следующая:

* Если необходимо подобрать только степень регуляризации для заданной структуры:
  * Если начальная точка не задана, то степень регуляризации подбирается методом *золотого сечения*, в противном случае — методом *схождения к вершине*.
* Если необходимо подобрать только структуру, не изменяя степень регуляризации:
  * Если не задана начальная структура, то она подбирается в два этапа: сначала происходит выбор количества скрытых слоев (0, 1 или 2), затем, если результат предыдущего этапа не 0, грубо подбирается размер скрытых слоев методом *золотого сечения*, причем для 2 скрытых слоев количество нейронов на данном этапе делается одинаковым.
  * Структура подбирается сразу по всем трем параметрам (число слоев, число нейронов) методом *схождения к вершине* из заданной либо подобранной начальной точки.
* Если необходим автоподбор структуры и регуляризации:
  * Подбирается структура так же, как и в предыдущем пункте. При этом, если начальное значение регуляризации задано, то используется оно, в противном случае — регуляризация отключена.
  * Если начальное значение регуляризации не было задано, оно подбирается методом *золотого сечения*.
  * Финальный этап автоподбора производится методом *схождения к вершине* по всем четырем параметрам.

  Блок-схема (граф переходов) реализованной стратегии автоподбора указана на рисунке:

![Алгоритм автоподбора](./autofit-neural-network-1.svg)
*Рисунок 1. Алгоритм автоподбора*
